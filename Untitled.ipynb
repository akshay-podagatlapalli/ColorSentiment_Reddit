{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e4225371",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import praw\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from transformers import pipeline\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from datetime import datetime\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "05175ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API CALL stuff\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"qJsvLNd6Goxgqz4LZ6yEbg\",\n",
    "    client_secret=\"KIUx33wL7W434pIy4lru06slhDjAEQ\",\n",
    "    user_agent=\"posts_ds\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "254bd8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the subreddit as a user input\n",
    "subreddit = reddit.subreddit(\"history\")\n",
    "\n",
    "# Add additional variables with different selections\n",
    "# like \"new_posts\", \"hot_posts\", \"controversial_posts\" etc... as user inputs\n",
    "# Add the limit as a user input\n",
    "new_posts = list(subreddit.controversial(limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c74f6435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function is used to fetch the data \n",
    "# from the specific subreddit and return them \n",
    "# in a dictionary\n",
    "def get_data(new_posts=new_posts):\n",
    "    data_dict = {}\n",
    "    for post in new_posts:\n",
    "        post_title = post.title\n",
    "        post.comments.replace_more(limit=None)\n",
    "        comments = [comment.body for comment in post.comments.list()]\n",
    "        if post_title in data_dict:\n",
    "            data_dict[post_title].extend(comments)\n",
    "        else:\n",
    "            data_dict[post_title] = comments\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "64b40de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time(new_posts = new_posts):\n",
    "    dict_time = {}\n",
    "    for post in new_posts:\n",
    "        post_title = post.title\n",
    "        post.comments.replace_more(limit = None)\n",
    "        time = [datetime.fromtimestamp(comment.created_utc).strftime('%Y-%m-%d %H:%M:%S') for comment in post.comments.list()]\n",
    "\n",
    "        if post_title in dict_time:\n",
    "            dict_time[post_title].extend(time)\n",
    "        else:\n",
    "            dict_time[post_title] = time\n",
    "    return dict_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c7f1ecf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function does the preprocessing of the text\n",
    "def clean_text(text, replacement_text=''):\n",
    "    # first remove URLS\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+|gif\\S+')\n",
    "    text_without_urls = url_pattern.sub(replacement_text, text)\n",
    "    # second remove, reddit specific tokens\n",
    "    reddit_tokens = re.compile(r'\\br/\\w+|\\bu/\\w+')\n",
    "    text_without_tokens = reddit_tokens.sub(replacement_text, text_without_urls)\n",
    "    # remove all special characters except emojis\n",
    "    special_char = re.compile(\n",
    "        r'[^\\w\\s.,!?]|(?<![\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F700-\\U0001F77F\\U0001F780-\\U0001F7FF\\U0001F800-\\U0001F8FF\\U0001F900-\\U0001F9FF\\U0001FA00-\\U0001FA6F\\U0001FA70-\\U0001FAFF\\U00002702-\\U000027B0\\U0001F1E6-\\U0001F1FF])')\n",
    "    cleaned_text = special_char.sub(replacement_text, text_without_tokens)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "58ab6f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_preprocess(data):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    clean_data_dict = {}\n",
    "\n",
    "    for title, comments in data.items():\n",
    "        clean_comments = []\n",
    "        for comment in comments:\n",
    "            # Remove URLs, special characters, etc.\n",
    "            comment = clean_text(comment)\n",
    "            # Remove all punctuation from the comments\n",
    "            comment = comment.translate(str.maketrans('', '', string.punctuation))\n",
    "            # convert all words to lower case\n",
    "            comment = comment.lower()\n",
    "            # tokenize all the words\n",
    "            word_tokens = word_tokenize(comment)\n",
    "            # add the tokenized words into a list\n",
    "            filtered_sentence = [w for w in word_tokens if w not in stop_words]\n",
    "            clean_comments.append(filtered_sentence)\n",
    "        \n",
    "        # Assign processed comments to the corresponding title\n",
    "        clean_data_dict[title] = clean_comments\n",
    "\n",
    "    return clean_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "afb3b6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the preprocessing and \n",
    "# and store all the cleaned data inside \n",
    "# a dataframe\n",
    "def fetch_and_process():\n",
    "\n",
    "    unclean_data_dict = get_data()\n",
    "    time_dict = get_time()\n",
    "    processed_data = init_preprocess(unclean_data_dict)\n",
    "\n",
    "\n",
    "    # Create the desired structure\n",
    "    comments_dict = {title: [[ \" \".join(comment) ] for comment in comments] \n",
    "                    for title, comments in processed_data.items()}\n",
    "\n",
    "    sentiment_pipeline = pipeline(\"sentiment-analysis\", \n",
    "                                model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "\n",
    "    emotion_pipeline = pipeline(\"text-classification\", \n",
    "                                model=\"bhadresh-savani/distilbert-base-uncased-emotion\", return_all_scores=True)\n",
    "\n",
    "    # Initialize empty list to store the results\n",
    "    results_sentiment_pipeline = []\n",
    "    results_emotion_pipeline = []\n",
    "\n",
    "    # Iterate through each title and its corresponding comments\n",
    "    for title, comments in comments_dict.items():\n",
    "        for idx, sub_comment in enumerate(comments, start=1):\n",
    "            result1 = sentiment_pipeline(sub_comment[0])  # Use the first item in the list (joined comment)\n",
    "            result2 = emotion_pipeline(sub_comment[0])\n",
    "            sentiment_score = result1[0]['score']  # Use 'label' (positive/negative) or 'score' for numeric sentiment\n",
    "            emotion = result2[0]\n",
    "            highest_emotion = max(emotion, key=lambda x: x['score'])\n",
    "            emotion_type = highest_emotion['label']\n",
    "            emotion_score = highest_emotion['score']\n",
    "\n",
    "            # Fetch the corresponding time from time_dict\n",
    "            comment_time = time_dict[title][idx - 1]  # Subtract 1 since enumerate starts from 1\n",
    "\n",
    "            # Append the data to the results list with time\n",
    "            results_sentiment_pipeline.append([title, f\"comment #{idx}\", sentiment_score, comment_time])\n",
    "            results_emotion_pipeline.append([title, f\"comment #{idx}\", emotion_type, emotion_score, comment_time])\n",
    "\n",
    "\n",
    "    # Create DataFrames with the additional time column\n",
    "    sent_data = pd.DataFrame(results_sentiment_pipeline, columns=[\"title\", \"comment_number\", \"result\", \"time\"])\n",
    "    sent_data.to_csv('data/output.csv', index=False)\n",
    "\n",
    "    emotion_data = pd.DataFrame(results_emotion_pipeline, columns=[\"title\", \"comment_number\", \"type\", \"score\", \"time\"])\n",
    "    emotion_data.to_csv('data/output2.csv', index=False)\n",
    "\n",
    "    return sent_data, emotion_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "91b2f626",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "c:\\Users\\aksha\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(                                                 title comment_number  \\\n",
       " 0    I can’t believe I didn’t learn about this in s...     comment #1   \n",
       " 1    I can’t believe I didn’t learn about this in s...     comment #2   \n",
       " 2    I can’t believe I didn’t learn about this in s...     comment #3   \n",
       " 3    I can’t believe I didn’t learn about this in s...     comment #4   \n",
       " 4    I can’t believe I didn’t learn about this in s...     comment #5   \n",
       " ..                                                 ...            ...   \n",
       " 546               Reframing the Thanksgiving Narrative    comment #21   \n",
       " 547               Reframing the Thanksgiving Narrative    comment #22   \n",
       " 548               Reframing the Thanksgiving Narrative    comment #23   \n",
       " 549               Reframing the Thanksgiving Narrative    comment #24   \n",
       " 550               Reframing the Thanksgiving Narrative    comment #25   \n",
       " \n",
       "        result                 time  \n",
       " 0    0.419607  2022-08-27 13:18:04  \n",
       " 1    0.851098  2022-08-27 15:38:08  \n",
       " 2    0.268647  2022-08-27 12:39:07  \n",
       " 3    0.331666  2022-08-27 11:36:45  \n",
       " 4    0.613351  2022-08-27 15:12:11  \n",
       " ..        ...                  ...  \n",
       " 546  0.518578  2023-11-19 23:45:43  \n",
       " 547  0.424112  2023-11-10 22:59:57  \n",
       " 548  0.278630  2023-11-15 13:31:10  \n",
       " 549  0.340275  2023-11-11 16:53:00  \n",
       " 550  0.420615  2023-11-11 16:53:01  \n",
       " \n",
       " [551 rows x 4 columns],\n",
       "                                                  title comment_number  \\\n",
       " 0    I can’t believe I didn’t learn about this in s...     comment #1   \n",
       " 1    I can’t believe I didn’t learn about this in s...     comment #2   \n",
       " 2    I can’t believe I didn’t learn about this in s...     comment #3   \n",
       " 3    I can’t believe I didn’t learn about this in s...     comment #4   \n",
       " 4    I can’t believe I didn’t learn about this in s...     comment #5   \n",
       " ..                                                 ...            ...   \n",
       " 546               Reframing the Thanksgiving Narrative    comment #21   \n",
       " 547               Reframing the Thanksgiving Narrative    comment #22   \n",
       " 548               Reframing the Thanksgiving Narrative    comment #23   \n",
       " 549               Reframing the Thanksgiving Narrative    comment #24   \n",
       " 550               Reframing the Thanksgiving Narrative    comment #25   \n",
       " \n",
       "         type     score                 time  \n",
       " 0      anger  0.810326  2022-08-27 13:18:04  \n",
       " 1       fear  0.650061  2022-08-27 15:38:08  \n",
       " 2        joy  0.731237  2022-08-27 12:39:07  \n",
       " 3      anger  0.692859  2022-08-27 11:36:45  \n",
       " 4      anger  0.546586  2022-08-27 15:12:11  \n",
       " ..       ...       ...                  ...  \n",
       " 546      joy  0.998568  2023-11-19 23:45:43  \n",
       " 547      joy  0.851946  2023-11-10 22:59:57  \n",
       " 548      joy  0.989219  2023-11-15 13:31:10  \n",
       " 549  sadness  0.862862  2023-11-11 16:53:00  \n",
       " 550      joy  0.812872  2023-11-11 16:53:01  \n",
       " \n",
       " [551 rows x 5 columns])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_it = fetch_and_process()\n",
    "get_it"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
